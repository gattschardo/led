% -*- mode: Noweb; noweb-code-mode: erlang-mode -*-

\documentclass{scrartcl}
\usepackage{noweb}

\title{led - Literate [[ed]]}
\author{Richard ``[[gattschardo]]'' Molitor}
\date{June 19, 2011 - \today} % this will eventually make sense

\begin{document}

\maketitle

\tableofcontents

\section{Introduction}

Welcome to [[led]]! First, let's make clear what [[led]] is - it stands for
``literate [[ed]]'', where [[ed]] of course is the name of the standard text
editor (on \textsc{Unix} anyways). So [[led]] is just a clone of the original
[[ed]] -- whether it ever becomes fully functional only the future will show.

However it's not the primary goal of [[led]] to become a fully operational
[[ed]] clone, -- use the \textsc{Gnu} or some \textsc{Bsd} variant for that. So,
what is the purpose of this thing? This is where the ``literate'' part comes in:
It's just an experiement with literate programming -- I want to see whether this
approach to programming works out for me or not. Also, [[led]] is meant to teach
me some more things about [[ed]] itself (which is a quite nice
program)\footnote{To be true to my own word, I should actually be writing this
in [[ed]], but well, shame on me, looks like I'll be using \textsc{Vim},
mainly for its nice [[make]] integration\footnote{Looks like I'm using
\textsc{Emacs}, now}\footnote{Actually, I just wanted to nest
  footnotes}}, and also about programming in
Erlang, since that is the language in which it is implemented. Especially, one
of the goals is to figure out \textsc{Yecc}, the Erlang version of the well known
[[yacc]] parser generator.

%\newpage

\section{The Big Picture}

Since [[ed]] is a fairly simple program (which is of course among the reasons
I choose it for this experiement), it is split into five parts, each of which
is then described in it's own section:

\begin{description}

\item         [the scanner]
tokenizes the input for the parser.

\item        [the parser]
will parse lines of input into an abstract syntax representation (generated
from \textsc{Yecc} grammar).

\item        [the buffer]
is the abstract data structure modeling the edit buffer, supporting operations
such as [[get]], [[append]] and [[change]].
<<buffer head>>=
-module(ed_buffer).
@

\item         [main]
will do the rest, that is, reading and writing buffers to file, initializing
the parser and buffer, shutting everything down
<<main head>>=
-module(ed_main).

-compile(export_all).% as above
@

\item        [the startup script]
will start the Erlang \textsc{VM} and pass in the command line arguments
(like, the file name) to main.
<<script head>>=
#!/bin/sh
@

\end{description}

\section{The Scanner}

The scanner must tokenize the input coming from an input port into an array
containing tuples of the form [[{Category, Line, Symbol}]] or, in case of
one-element categories [[{Symbol, Line}]].

At first I believed that I could get away with using the [[scan_erl_form]]
from the [[io]] module and post-processing it's output a bit, but this is not
going to work well, because I cannot have whitespace filtered out in regular
expressions, for example.

So, now there is my own\footnote{Brian Kernighan says, that he always writes
his scanners himself, too, so I guess I'm not in bad company} -- it always
only reads from \textit{stdin}, which should be appropriate for [[ed]]:

<<scan>>=
scan() ->
    case io:get_line( "led> " ) of
        eof ->
            { eof, 1 };
        { error, Reason } ->
            { error, Reason, 1 };
        Line ->
            { ok, lex( Line ), 1 }
    end.
@

The [[scan/0]] function tries to read a line, and calls [[lex/1]] on it if it
can get one. Our prompt for now is [[led>]], which is of course incorrect --
I'll use an empty prompt once I'm convinced that my clone is accurate enough.
The 1 which is always the last member of our tuple stands for the line number,
but since I don't care about multiple lines for now, it's just constant. I'm
not sure whether I will use it later or remove it entirely.

<<lex>>=
lex( Tokens ) ->
    lex( Tokens, []).

lex( [], Result ) ->
    lists:reverse( Result );
lex( [ Token | Ts ]= In, Result ) ->
    Digit   = is_digit( Token ),
    Special = is_special( Token ),
    Action  = is_action( Token ),
    White   = is_whitespace( Token ),
    if
        <<token type switch>>
    end.
@

This scanner is fairly simple-minded -- it never processes more than one
digit, unless it finds an integer. Since we cannot have arbitrary functions in
an Erlang if-statement, my solution for now is to call all these ''guards''
beforehand and just switch on their boolean results afterwards.

There are four conditions we are interested in:

\begin{description}

\item [digits:]
when we find a digit, we can simply let [[string:to_integer/1]] do the
conversion work, since it conveniently spits out the rest of the string after
the end of our integer:
<<token type switch>>=
Digit ->
    { Int, Rest } = string:to_integer( In ),
    lex( Rest, [{ integer, 1, Int }| Result ]);
@

\item [specials:]
these are some punctuation marks that ed treats specially in commands, like
the dot ``.'' or ``\$''. We convert them to atoms, to get rid of numeric
\textsc{Ascii} values:
<<token type switch>>=
Special ->  
    lex( Ts, [{ list_to_atom([ Token ]), 1 }| Result ]);
@

\item [actions:]
those are the letters that can be ed commands, like ``w'', ``q'', ``p''. We
get an atom that describes them a bit better, like:
<<token for p>>=
get_token( 112 ) -> % ascii 'p'
    print_tok;
@

and add that as a token:
<<token type switch>>=
Action ->
    lex( Ts, [{ get_token( Token ), 1, Token }| Result ]);
@

\item [whitespace]
is just \textit{space} and \textit{tab} here, because \textit{newline} is
already special, we just mark them:
<<token type switch>>=
White ->
    lex( Ts, [{ whitespace, 1, Token }| Result ]);
@

\end{description}

Everything else goes in the catch-all category ``letter'' as is:
<<token type switch>>=
true ->
    lex( Ts, [{ letter, 1, Token }| Result ])
@

Now all that remains to be done, is to define the classification functions
like [[is_digit/1]] and the [[get_token/1]] which gives the tokens for our
commands.

For digits it suffices to check the \textsc{Ascii} codes for 0 and 9 for now
(Erlang has no representation for single characters, so I'm giving their
numeric values, 48 and 57).

<<lex>>=
is_digit(Char) ->
    if
        Char >= 48, Char =< 57 ->
            true;
        Char < 48; Char > 57 ->
            false
    end.
@ 

[[is_special/1]], [[is_whitespace/1]] and [[is_action/1]] simply check for
\textsc{Ascii} codes:

<<classifiers>>=
is_special( 44 ) -> % ,
    true;
is_special( 10 ) -> % \n
    true;
is_special( 46 ) -> % .
    true;
is_special( 36 ) -> % $
    true;
is_special( 45 ) -> % -
    true;
is_special( 43 ) -> % +
    true;
is_special( _ ) ->
    false.

is_whitespace( 9 ) -> % \t
    true;
is_whitespace( 32) -> % space
    true;
is_whitespace( _ ) -> 
    false.

is_action( 97 ) -> % a
    true;
is_action( 99 ) -> % c
    true;
is_action( 100 ) -> % d
    true;
is_action( 101 ) -> % e
    true;
is_action( 103 ) -> % g
    true;
is_action( 105 ) -> % i
    true;
is_action( 106 ) -> % j
    true;
is_action( 112 ) -> % p
    true;
is_action( 113 ) -> % q
    true;
is_action( 114 ) -> % r
    true;
is_action( 115 ) -> % s
    true;
is_action( 117 ) -> % u
    true;
is_action( 119 ) -> % w
    true;
is_action( 120 ) -> % x
    true;
is_action( 121 ) -> % y
    true;
is_action( _ ) ->
    false.
@

And finally [[get_token/1]]:
<<tokens>>=
get_token( 97 ) -> % a
    append_tok;
get_token( 99 ) -> % c
    change_tok;
get_token( 100 ) -> % d
    delete_tok;
get_token( 101 ) -> % e
    edit_tok;
get_token( 103 ) -> % g
    global_tok;
get_token( 105 ) -> % i
    insert_tok;
get_token( 106 ) -> % j
    join_tok;
<<token for p>>
get_token( 113 ) -> % q
    quit_tok;
get_token( 114 ) -> % r
    read_tok;
get_token( 115 ) -> % s
    replace_tok;
get_token( 117 ) -> % u
    undo_tok;
get_token( 119 ) -> % w
    write_tok;
get_token( 120 ) -> % x
    paste_tok;
get_token( 121 ) -> % y
    yank_tok.
@ 

Another thing that I decided to put into the scanner (as it is also concerned
with reading user input) is the function to read chunks ended by [[.]] on a
line by itself. if will just return it as a list of strings, one for each
line:

<<read chunk>>=
read_chunk() ->
    Ch = read_chunk([]),
    %io:format ("Chunk is ~p, reversing~n", [ Ch ]),
    lists:reverse( Ch ).

read_chunk( Ls ) ->
    Termination = ".\n",
    case io:get_line( "chunk> " ) of
        eof ->
            Ls;
        { error, _Reason } ->
            Ls;
        Line ->
	    %io:format ("Read ~p~n", [ Line ]),
            if
		Line == Termination ->
		    Ls;
		Line /= Termination ->
		    read_chunk([ Line | Ls ])
	    end
    end.
@

That's it, now lets bring it together in one file:

<<[[ed_scanner.erl]]>>=
-module(ed_scanner).

-compile(export_all).

<<scan>>
<<lex>>
<<classifiers>>
<<tokens>>
<<read chunk>>
@

\section{The Parser}

Ok, so let's start defining the grammar for the parser. Of course, in practice
it may not be very rewarding to write a grammar for [[ed]] input, since it's
not very complex and a hand-coded parser should work fine. But remember, I'm
trying to learn \textsc{Yecc}, here\footnote{Also note that Pippijn says a formal
grammar is always a good thing to have, since it will always be easier to
maintain than a hand-written parser}.

The terminal symbols for the grammar are as follows:

<<parser terminals>>=
Terminals 
% specials:
    ',' '\n' '.' '$' '-' '+'
% categories:
    'integer' 'letter' 'whitespace'
% tokens
    'append_tok' 'change_tok' 'delete_tok'  'edit_tok'
    'global_tok' 'insert_tok' 'join_tok'    'print_tok'
    'quit_tok'   'read_tok'   'replace_tok' 'undo_tok'
    'write_tok'  'paste_tok'  'yank_tok'.
@

Note that [[integer]] and [[atom]] are provided by [[erl_scan]] and are simply
Erlang integers resp. atoms.

The nonterminals that we will need are the following

<<parser nonterminals>>=
Nonterminals
% general
    input command address region filename space regex
% commands
    append change delete edit global insert join print 
    quit read replace undo write paste yank.
@

We will of course be parsing commands one by one, but in non-interactive mode
there can of course be several at once:

<<parser root>>=
Rootsymbol input.
@

Actually, this might not be what I need, though, the question is how the
parsing works, that is, whether is parses incomplete input. If not, I'll need
to restart a new parser for each command, and then of course, the root symbol
can just be [[command]] instead of [[commands]].

To get a general idea how \textsc{Yecc} input looks like, here is the print
action:

<<parser print>>=
print ->
    region 'print_tok' :
        {print, '$1'}.
print ->
    'print_tok' :
        {print, [region, {'.', 1}]}.
@

<<parser rules>>=
input ->
    command '\n' :
        '$1'.

command ->
    append :
        '$1'.
command ->
    change :
        '$1'.
command ->
    delete :
	'$1'.
command ->
    edit :
	'$1'.
command ->
    global :
	'$1'.
command ->
    insert :
	'$1'.
command ->
    join :
	'$1'.
command ->
    print :
        '$1'.
command ->
    quit :
        '$1'.
command ->
    read :
	'$1'.
command ->
    replace :
	'$1'.
command ->
    undo :
	'$1'.
command ->
    write :
	'$1'.
command ->
    paste :
	'$1'.
command ->
    yank :
	'$1'.

address ->
    'integer' :
	[address, '$1'].
address ->
    '.' :
	[address, '$1'].
address ->
    '$' :
	[address, '$1'].
address ->
    '-' 'integer' :
        [address, '$1', '$2'].
address ->
    '+' 'integer' :
        [address, '$1', '$2'].
address ->
    '-' :
        [address, '$1', {integer, 1, 1}].
address ->
    '+' :
        [address, '$1', {integer, 1, 1}].

region ->
    address ',' address :
        [region, '$1', '$2', '$3'].

space ->
    'whitespace' space :
	['$1'|'$2'].
space ->
    'whitespace' :
	['$1'].

filename ->
    'letter' :
	{filename, ['$1']}.

regex ->
    'append_tok' :
	{regex, ['$1']}.

append ->
    address 'append_tok' :
	{append, '$1'}.

change ->
    region 'change_tok' :
	{change, '$1'}.

delete ->
    region 'delete_tok' :
	{delete, '$1'}.

edit ->
    'edit_tok' space filename :
	{edit, '$3'}.

global -> % addr?
    'global_tok' regex :
	{global, '$2'}.

insert ->
    region 'insert_tok' :
	{insert, '$1'}.

join ->
    region 'join_tok' :
	{join, '$1'}.

<<parser print>>

quit ->
    'quit_tok' :
        {quit, []}.

read ->
    'read_tok' space filename :
	{read, '$3'}.

replace ->
    'replace_tok' regex filename :
	{replace, ['$2'|'$3']}.

undo ->
    'undo_tok' :
	{undo, []}.

write ->
    'write_tok' :
	{write, []}.
write ->
    'write_tok' space filename :
	{write, '$3'}.

paste ->
    region 'paste_tok' :
	{paste, '$1'}.

yank ->
    region 'yank_tok' :
	{yank, '$1'}.
@

So, all in all, the parser becomes:
<<[[ed_parser.yrl]]>>=
<<parser nonterminals>>
<<parser terminals>>
<<parser root>>
<<parser rules>>
@

\section{The Buffer Data Structure}

Ok, time to loose some words about what the buffer needs to do.
The manipulations of the different commands fall into a few simple categories:

\begin{itemize}

\item
getting a region from the buffer

\item
changing (i.e. replacing) a region from the buffer with someting else

\item
appending after or inserting before a given address.

\end{itemize}

For creation we'll just allow creating empty buffers. Reading stuff from file
will be the job of the read command, which we'll then call implicitly at
startup if we have a filename argument on the command line. We'll also need
some operation to figure out how long it is.

this leads to the following function exports:
<<buffer exports>>=
-export([new/0, get/2, set/3, put/3, len/1]).
@

Though it might not be efficient, it is certainly clearest to modify the
buffer as a list of strings (that is, a list of lists) for now. Each string
represents one line, so they should all be newline-terminated (except perhaps
the last).

An empty buffer as returned by [[new/0]] is therefore just a tag indicating
that this really is a buffer, and an empty list:
<<buffer new>>=
new() ->
    {buffer, []}.
@

Pretty simple, huh? Getting a region is also not very hard, once we have
figured out how to present a region. We'll just go with absolute addresses
here, and do the computation of what [[.]] or [[$]] is somewhere else (this
can be computed using [[length/1]]). This may not be sufficient for regular
expressions, but we'll take care of that later.

<<buffer get>>=
len({buffer, L}) when is_list(L) ->
    len(L).

get({buffer, Ls}, {region, First, Last}) ->
    F = fun(Line,{Pos,Res}=Acc) ->
	    %io:format( "~p < ~p < ~p?~n", [ First, Pos, Last ]),
	    if
		Pos < First ->
		    %io:format( "Discarding1 ~p~n", [ Line ]),
		    {Pos+1,Res};
		Pos >= First, Pos =< Last ->
		    %io:format( "Adding ~p~n", [ Line ]),
		    {Pos+1,[Line|Res]};
		Pos > Last ->
		    %io:format( "Discarding2 ~p~n", [ Line ]),
		    {Pos+1,Res};
		true ->
		    io:format( "~p < ~p < ~p?~n", [ First, Pos, Last ]),
		    Acc
	    end
	end,
    { _, R } = lists:foldl(F, {1,[]}, Ls),
    lists:reverse(R).
@

Putting just means inserting new text after the specified line. We allow
putting at line 0 to insert to the beginning. So put does essentially the same
as the [[a]]-command.

<<buffer put>>=
put({ buffer, Ls } = _Buf, { address, A }, Chunk) when A >= 0 ->
    %true = len(Buf) >= A,
    %io:format( "appending ~p (~p)~n", [ Chunk, A ]),
    F = fun( L, { Pos , Res } = _Acc) ->
	    if
		Pos == A ->
		    %io:format( "APPending ~p~n", [ Chunk ]),
		    { Pos+1, [ L, Chunk | Res ]};
		Pos /= A ->
		    %io:format( "~p not right ~p~n", [ Pos, A ]),
		    { Pos+1, [ L | Res ]}
	    end
	end,
    { Len, R } = lists:foldl( F, { 0, []}, Ls ),
    if
	Len == A ->
	    { buffer, Ls ++ Chunk };
	Len /= A ->
	    { buffer, lists:reverse( R )}
    end.
@

Setting on the other hand means taking out a region and replacing it by
something else -- similar to what the [[c]]- and [[s]]-commands do.

<<buffer set>>=
set({buffer, Ls}=_Buf,{region, First, Last},Chunk) ->
    F = fun(L,{Pos,Res}) ->
	    if
		Pos < First ->
		    {Pos+1,[L|Res]};
		Pos == First ->
		    {Pos+1,[Chunk|Res]};
		Pos > First, Pos =< Last ->
		    {Pos+1,Res};
		Pos > Last ->
		    {Pos+1,[L|Res]}
	    end
	end,
    { _, R } = lists:foldl(F, {1,[]}, Ls),
    {buffer,lists:reverse(R)}.
@

<<[[ed_buffer.erl]]>>=
<<buffer head>>
<<buffer exports>>
<<buffer new>>
<<buffer get>>
<<buffer put>>
<<buffer set>>
@

\section{The Main Module}

For now, the main module just parses lines of input and either prints
the resulting syntax tree, or exits.

<<main parse>>=
start() ->
    loop({ state, ed_buffer:new(), 0 }).

loop(State) ->
    case step( State ) of
        { true, NewState } ->
            loop( NewState );
        false ->
            "Bye"
    end.

step( State ) ->
    case ed_scanner:scan() of
	{ ok, T, 1 } ->
	    step_process( T, State );
	{ eof, _ } ->
	    io:put_chars( "\nNot Ignoring EOF, also quit with q\n" ),
	    false;
	Crap ->
	    io:format( "Lexer talking shit: ~p~n", [ Crap ]),
	    false
    end.

step_process( T, State ) ->
    case ed_parser:parse( T ) of
        { ok, S } ->
            if
                is_tuple( S ), S == { quit, [] } ->
                    false;
                true ->
                    io:format( "S-Tree: ~p~n", [ S ]),
		    NewState=eat( S, State ),
                    { true, NewState }
            end;
        { error, { _Line, ed_parser, [ Err, Message ]}} ->
            io:format( "? ~s~s~n", [ Err, Message ]),
            { true, State }
    end.
@ 

<<main eat>>=
% will be moved out later
eat({ Command, Args },{ state, Buf, Dot } = State) ->
    case Command of
	append ->
	    Ch = ed_scanner:read_chunk(),
	    [ address, {integer, 1, Addr }] = Args,
	    NewBuf = ed_buffer:put( Buf, { address, Addr }, Ch),
	    { state, NewBuf, Dot };
	print ->
	    [region,[address,{integer,1,First}],_,[address,{integer,1,Last}]] = Args,
	    Out = ed_buffer:get( Buf, {region, First, Last} ),
	    %io:format( "Out is ~p~n", [ Out ]),
	    if
		Out /= [] ->
		    io:put_chars( Out );
		Out == [] ->
		    io:format ( "buffer is ~p~n", [ Buf ])
	    end,
	    State;
	_ ->
	    io:format( "Do Nothing!~n", []),
	    State
    end.
@

<<[[ed_main.erl]]>>=
<<main head>>
<<main parse>>
<<main eat>>
@

\section{The Startup Script}

Just start the main method (without passing any arguments, for now).

<<[[led.sh]]>>=
<<script head>>

echo "Hello, led!"
erl -noshell -pa ~/code/led -s ed_main start -s init stop
@

\section{Conclusion}

It's fun, try it!

\section{Defined Chunks}

\nowebchunks

%\section{Index}
%
%\nowebindex

\end{document}

% vim: se tw=78 sw=4 sts=4:
